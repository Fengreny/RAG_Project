{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "分步说明：\n",
    "\n",
    "1. 用户输入自然语言查询\n",
    "2. 系统先去检索跟查询相关的表\n",
    "3. 根据表的 Schema 让大模型生成 SQL\n",
    "4. 用生成的 SQL 查询数据库\n",
    "5. 根据查询结果，调用大模型生成自然语言回复"
   ],
   "id": "bd763552c91ad7ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c3f569402f771734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "529d80808474b63b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. 数据遍历，当前的目录下",
   "id": "45a8608350b33bfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:04:29.476629Z",
     "start_time": "2025-11-11T13:04:29.429799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./WikiTableQuestions/csv/200-csv\")\n",
    "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"processing file: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {csv_file}: {str(e)}\")"
   ],
   "id": "ee1bc04217f67fba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: WikiTableQuestions\\csv\\200-csv\\0.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\1.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\10.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\11.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\12.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\14.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\15.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\17.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\18.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\20.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\22.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\24.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\25.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\26.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\28.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\29.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\3.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\30.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\31.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\32.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\33.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\34.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\35.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\36.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\37.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\38.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\4.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\41.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\42.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\44.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\45.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\46.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\47.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\48.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\7.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\8.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\9.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. 为每个表生成一段文字表述（用于检索），保存在 `WikiTableQuestions_TableInfo` 目录",
   "id": "bded21239e8bd252"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:04:32.620241Z",
     "start_time": "2025-11-11T13:04:31.361614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\n",
    "Give me a summary of the table with the following JSON format.\n",
    "\n",
    "- The table name must be unique to the table and describe it while being concise.\n",
    "- Do NOT output a generic table name (e.g. table, my_table).\n",
    "\n",
    "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
    "\n",
    "Table:\n",
    "{table_str}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[ChatMessage.from_str(prompt_str, role=\"user\")]\n",
    ")"
   ],
   "id": "a1d992889ba26eda",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:04:33.416476Z",
     "start_time": "2025-11-11T13:04:33.403995Z"
    }
   },
   "cell_type": "code",
   "source": "tableinfo_dir = \"WikiTableQuestions_TableInfo\"",
   "id": "4273bbd5d93bf6d2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _get_tableinfo_with_index(idx: int) -> str:\n",
    "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
    "    results_list = list(results_gen)\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "    elif len(results_list) == 1:\n",
    "        path = results_list[0]\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return TableInfo.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"More than one file matching index: {list(results_gen)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "table_names = set()\n",
    "table_infos = []\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_info = _get_tableinfo_with_index(idx)\n",
    "    if table_info:\n",
    "        table_infos.append(table_info)\n",
    "    else:\n",
    "        while True:\n",
    "            df_str = df.head(10).to_csv()\n",
    "            table_info = llm.structured_predict(\n",
    "                TableInfo,\n",
    "                prompt_tmpl,\n",
    "                table_str=df_str,\n",
    "                exclude_table_name_list=str(list(table_names)),\n",
    "            )\n",
    "            table_name = table_info.table_name\n",
    "            print(f\"Processed table: {table_name}\")\n",
    "            if table_name not in table_names:\n",
    "                table_names.add(table_name)\n",
    "                break\n",
    "            else:\n",
    "                # try again\n",
    "                print(f\"Table name {table_name} already exists, trying again.\")\n",
    "                pass\n",
    "\n",
    "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
    "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
    "    table_infos.append(table_info)"
   ],
   "id": "f59c166ccb252ebc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
